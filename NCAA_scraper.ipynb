{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3a525-826f-40c5-b578-137aa120b531",
   "metadata": {
    "id": "bdd3a525-826f-40c5-b578-137aa120b531",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import regex as re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0829e7-c828-4de1-b7ae-02e35f3ebd1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "ad0829e7-c828-4de1-b7ae-02e35f3ebd1c",
    "outputId": "8e320f86-13f4-4f65-c8d1-cfa42c61c4e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NCAAFH_SCRAPE(start_date,season_length_in_days = 90):\n",
    "    # generate a list of all of the dates in the season\n",
    "    date_list = [start_date + datetime.timedelta(days=x) for x in range(season_length_in_days)]\n",
    "    \n",
    "    ## itterates the list of dates and retrives all compiles a url for each day\n",
    "    pages = [f\"http://stats.ncaa.org/season_divisions/17902/livestream_scoreboards?utf8=âœ“&season_division_id=&game_date={date_list[x].month}%2F{date_list[x].day}%2F2022&conference_id=0&tournament_id=&commit=Submit\" for x,n in enumerate(date_list)]\n",
    "    \n",
    "    print('dates parsed')\n",
    "    print('finding games')\n",
    "    # loop to itterate through url generated above and retrive all box scores to all games on that day\n",
    "    \n",
    "    box_scores = [] \n",
    "    for page in tqdm(pages):\n",
    "    \n",
    "        url = page\n",
    "    \n",
    "        header = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "        }\n",
    "    \n",
    "        response = requests.get(url,headers=header)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Find all table rows\n",
    "        rows = soup.find_all('tr')\n",
    "        \n",
    "        res = [row.find_all('a',string='Box Score') for row in rows]\n",
    "        res = list(filter(lambda a: a != [], res))\n",
    "        \n",
    "        box_scores+=([f\"http://stats.ncaa.org{el[0].get('href')}\" for el in res]) # make list of urls to box scores\n",
    "    \n",
    "    \n",
    "    all_games = [] # define list\n",
    "    \n",
    "    print('games found')\n",
    "    \n",
    "    for target in tqdm(box_scores): # itterate through list of box score addresses\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f'retriving game data for: {target}')\n",
    "        \n",
    "        header = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "            } # ncaa will block if there is no user agent\n",
    "    \n",
    "        response = requests.get(target,headers=header)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        #pbp link\n",
    "        pbp_link = f\"http://stats.ncaa.org{soup.find('a',string='Play by Play').get('href')}\"\n",
    "    \n",
    "        # get date of game\n",
    "        date = soup.find_all('table')[2].find('td',string='Game Date:').next_sibling.next_sibling.text.strip()\n",
    "        \n",
    "        # Find the table element\n",
    "        tables = soup.find_all('table', class_='mytable')\n",
    "    \n",
    "        # Create an empty list to store the data\n",
    "        datas = []\n",
    "        datah = []\n",
    "        dataa = []\n",
    "    \n",
    "        # Find all rows in the three tables\n",
    "        rowss = tables[0].find_all('tr')\n",
    "        rowsh = tables[1].find_all('tr')\n",
    "        rowsa = tables[2].find_all('tr')\n",
    "    \n",
    "        # Loop through each row and extract the cell values from the tables\n",
    "        for row in rowss:\n",
    "            cells = row.find_all('td')\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            datas.append(row_data)\n",
    "    \n",
    "        for row in rowsh:\n",
    "            cells = row.find_all('td')\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            datah.append(row_data)\n",
    "    \n",
    "        for row in rowsa:\n",
    "            cells = row.find_all('td')\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            dataa.append(row_data)\n",
    "    \n",
    "    \n",
    "        df_sum = pd.DataFrame(datas)\n",
    "        df_home = pd.DataFrame(datah)\n",
    "        df_away = pd.DataFrame(dataa)\n",
    "    \n",
    "        df_home = df_home[2:-1]\n",
    "        df_away = df_away[2:-1]\n",
    "    \n",
    "        target = pbp_link # make another request, retriving the play by play\n",
    "        response = requests.get(target,headers=header)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        tables = soup.find_all('table', class_='mytable')\n",
    "    \n",
    "        pbp = []\n",
    "        for table in tables: # get all rows from all tables\n",
    "    \n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [cell.text.strip() for cell in cells]\n",
    "                pbp.append(row_data)\n",
    "    \n",
    "        pbp = pbp[4:-1] # chop head and tail off\n",
    "    \n",
    "        pbp = list(filter(lambda a: len(a) > 1, pbp)) # only incude rows with more than one element\n",
    "        pbp = list(filter(lambda a: a[0] != 'Time', pbp)) # do not include column head rows\n",
    "    \n",
    "        data = {'url':target,'df_sum':df_sum,'df_home':df_home,'df_away':df_away,'pbp':pbp,'date':date} # assemble dict\n",
    "        \n",
    "        all_games.append(data) # add to list of dicts\n",
    "        \n",
    "    df = pd.DataFrame(all_games)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9e9f0-e211-49cf-a3bf-984cdde1d1b1",
   "metadata": {
    "id": "0dd9e9f0-e211-49cf-a3bf-984cdde1d1b1"
   },
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2022, 8, 26)\n",
    "season_length_in_days = 87\n",
    "\n",
    "df = NCAAFH_SCRAPE(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdL44FRWDnz5",
   "metadata": {
    "id": "zdL44FRWDnz5"
   },
   "outputs": [],
   "source": [
    "# Save data as a file\n",
    "path = '/Users/euan_brown/Documents/GitHub/NCAA-fh-project/results.pkl'\n",
    "\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
